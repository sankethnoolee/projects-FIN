spark.master=local



#Details to access databricks workspace
#without https
app.databricks.workspace.url = adb-3214543931293544.4.azuredatabricks.net
app.databricks.auth.token = dapid340a36a366b4b79c03990d363311a8d
#retries for any operation in general.
app.databricks.maxretries = 5
app.databricks.retry.interval.millis= 60000

#Details to access azure storage.
app.azure.blob.container.name = val-1601
app.azure.blob.storageaccount.name = fintellixpocstorage
app.azure.blob.sas.key.value = ?sv=2019-12-12&ss=b&srt=sco&sp=rwdlacx&se=2021-03-15T16:56:47Z&st=2021-01-15T08:56:47Z&spr=https&sig=luAHw%2BDSPJw9dzO2MVgYVpZUfbFvz8Qz%2BSg1T87Z7YM%3D

#Details for creating new cluster.
#refer https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters#clusterclusterservicelistsparkversions
app.databricks.cluster.runtime = 7.4.x-scala2.12
#refer https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters#clusterclusterservicelistnodetypes
#for RBL, recommended to use Standard_DS12_v2 type, below value is for PoC only.
app.databricks.cluster.nodetype = Standard_F4s

#In case auto scale is ON, then below are min and max workers, else, static cluster with default workers size.
app.databricks.cluster.autoscale = false
app.databricks.cluster.workers.default = 1
app.databricks.cluster.workers.max = 4

#job level properties
app.sparkapp.jar.localpath = ../spark-java-databricks-example/target/
app.sparkapp.jar.deploy.path = lib/

#this can be "n" jars
app.sparkapp.jar.name.1 = spark-java-databricks-example-1.0-SNAPSHOT.jar
#app.sparkapp.jar.name.2 ="some-utility.jar"

#additionally databricks allows users to specify maven dependency libraries and will make it avilable for app.
#app.sparkapp.maven.dependency.1 = "org.jsoup:jsoup:1.7.2"
#app.sparkapp.maven.dependency.2 = "org.jsoup:jsoup:1.7.2"


app.sparkapp.mainclass.name = com.verisk.sparkexample.SparkApp
app.sparkapp.maxretries = 3
app.sparkapp.timeout.seconds = 3600
app.sparkapp.polling.interval.seconds = 180

#Our spark app consumes 2 parameters, parameter 1 is file path for source file, parameter 2 is destination path.
#This can be a dbfs path such as dbfs:/data/src/mydatafile.csv or a mount path if we have mounted storage in dbfs
#we are taking approach of reading and writing into azure blob directly. for that, path should be of format
# "wasbs://<container-name>@<storage-account-name>.blob.core.windows.net/<directory-name>/<file name>"
#this can be "n" parameters.
app.sparkapp.parameter.1 = wasbs://val-1601@fintellixpocstorage.blob.core.windows.net/data-input/Restaurants_in_Wake_County_NC.csv
app.sparkapp.parameter.2 = wasbs://val-1601@fintellixpocstorage.blob.core.windows.net/data-output/

